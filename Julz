### A.Link 12.11 - Julz code for pre-post analysis
#I was watching the judiciary committee debate on the articles of impeachments while writing this, pardon if I have jumps in thought or seem inconsistent
pre.per=c(10,8,7,6,5,7);sum(pre.per);post.per=c(10,10,9,10,8,9);sum(post.per)
mean(pre.per);mean(post.per)#so this allows you to see your average, feel free to do that to each after.
pre.val=c(8,5,7,6,6,8);post.val=c(10,10,9,10,8,10)
pre.int=c(8,10,8,0,6,10);post.int=c(10,10,9,10,8,10)
pre.mot=c(10,3,7,10,7,10);post.mot=c(10,10,10,8,10,9)
pre.att=c(8,8,8,7,7,8);post.att=c(9,10,10,10,9,10)
pre.ide=c(8,7,7,6,7,7);post.ide=c(10,10,8,10,7,10)
pre.exp=c(7,9,7,7,7,8);post.exp=c(8,10,9,10,8,10)
pre.emo=c(8,7,7,3,8,9);post.emo=c(10,10,10,10,9,10)
pre.und=c(2,7,8,5,5,10);post.und=c(8,7,10,9,7,8)
pre.ret=c(10,10,7,3,5,10);post.ret=c(10,10,8,10,7,10)#check the environment, all have 1:6, so numbers still might be wrong (account for that)

#so data now is there, make into a frame to conduct an analysis
pre=c(10,8,7,6,5,7,8,5,7,6,6,8,8,10,8,0,6,10,10,3,7,10,7,10,8,8,8,7,7,8,8,7,7,6,7,7,7,9,7,7,7,8,8,7,7,3,8,9,2,7,8,5,5,10,10,10,7,3,5,10)
post=c(10,10,9,10,8,9,10,10,9,10,8,10,10,10,9,10,8,10,10,10,10,8,10,9,9,10,10,10,9,10,10,10,8,10,7,10,8,10,9,10,8,10,10,10,10,10,9,10,8,7,10,9,7,8,10,10,8,10,7,10)
x=data.frame(pre,post)
#okay, time to differenciate 
type=c(rep('per',6),rep('val',6),rep('int',6),rep('mot',6),rep('att',6),rep('ide',6),rep('exp',6),rep('emo',6),rep('und',6),rep('ret',6))
x$type=as.factor(type)

#suggest a summary package like rmisc
library(Rmisc)
p=group.CI(pre~type,x)#from this you can find your greatest and lowest pre-test
min(p[3]);max(p[3])#min=und, max=mot
pp=group.CI(post~type,x)#again, extract your highest,lowest
min(pp[3]);max(pp[3]) #min=und, max=emo

# try graphing - I think
library(ggplot2)
d1=position_dodge(width = 0.8)
dd=8

g1=ggplot(pp,
          aes(x=type,y=post.mean,fill=type))+
  geom_errorbar(aes(ymax=post.upper, ymin=post.lower), 
                width=0.2, size=0.5, color="black",position = d1)+
  geom_point( size=3, shape=21,position=d1)
g1
### present thoughts... its nice to see your assumed plotted data, but lets find the difference which better shows the effect
# going to edit the frame, add a column

#difference between pre-post (what most people care about): assumption of paired samples (check again for data, feel free to reasign values above in lines 2-12)
x$diff=x$post-x$pre #this will provide positives (if the treatment is effective - hypothesis/null of zero, correct?)

#so try graphing again - accounting for assumption
#test for assumption - seems a little left skewed
hist(x$diff) #I would run a non-parametric test, but consider the grouping effects too

plot(diff~type,x)#outliers of emo, inf, ref (not too bad though, in my opinion)
#chose not to transform, lose some data
p1=group.CI(diff~type,x)

g2=ggplot(p1,
          aes(x=type,y=diff.mean,fill=type))+
  geom_errorbar(aes(ymax=diff.upper,ymin=diff.lower),
                width=0.2,size=0.5,color='black',position=d1)+
  geom_point( size=3, shape=21,position=d1)+
  geom_hline(yintercept = 0,linetype='dashed',color='gray')+
  labs(x='Learning Objective',y=expression('Score effect size,CI 95'),size=dd)+
  theme(axis.ticks.x=element_blank())+
  theme_classic()+
  guides(fill=F)

g2+coord_flip()

### I feel like this is going pretty okay.

#my own assumption is that these treatment outcomes are actually indendent tests in the following analysis.

#Test learning type 
### second sitting: so, I think each learning objective should be tested for significance.
#First thing first, testing assumption of normality (differences, not data)
#shapiro.test outcomes, if a < .05, then non-normal
shapiro.test(x$diff[1:6])#normal
shapiro.test(x$diff[7:12])#non-normal,val
shapiro.test(x$diff[13:18])#non-normal,int
shapiro.test(x$diff[19:24])#normal
shapiro.test(x$diff[25:30])#normal
shapiro.test(x$diff[31:36])#normal
shapiro.test(x$diff[37:42])#normal
shapiro.test(x$diff[43:48])#normal
shapiro.test(x$diff[49:54])#normal
shapiro.test(x$diff[55:60])#non-normal,ret
###Now analysis: normal=paired t-test, non-normal wilcoxon rank sum - again, difference normality was tested, not pre-post test data
t.test(x$pre[1:6],x$post[1:6],paired = T)#norm, significant@<=0.05=per
wilcox.test(x$pre[7:12],x$post[7:12],paired = T)#non-norm, sig@<=0.05=val
wilcox.test(x$pre[13:18],x$post[13:18],paired = T)#non-norm, NON-sig@.09=int
t.test(x$pre[19:24],x$post[19:24],paired = T)#norm, NON-sig@.27=mot
t.test(x$pre[25:30],x$post[25:30],paired = T)#norm, sig@<=0.01=att
t.test(x$pre[31:36],x$post[31:36],paired = T)#norm, sig@<=0.02=ide
t.test(x$pre[37:42],x$post[37:42],paired = T)#norm, sig@<=0.01=exp
t.test(x$pre[43:48],x$post[43:48],paired = T)#norm, sig@<=0.05=emo
t.test(x$pre[49:54],x$post[49:54],paired = T)#norm, NON-sig@.14=und
wilcox.test(x$pre[55:60],x$post[55:60],paired = T)#non-normal,NON-sig@.18=ret

###Okay, lets see if I can get something out of the groups = predictability power of some sort, trying to relate to your original question of which is greater or less
#I think focusing on the differences will help your argument, if you think differently, feel free to intervene
#thinking a repeated anova,ancova, or mixed model.
#pre-post, controlling for pre...this is change between
ancov=aov(post~pre+type,x)
summary(ancov) #there is no significant difference between learning outcomes...

#so, in sense of your comments, there really is no way to say there is a greater or lesser impact between outcomes (both pre/post)

###Answers to Julz' boss (12.13):
#Run code in program R

#Descriptive stats (most basic overview of data outcomes): 
#pre values ranked + highest/- lowest, based in total or mean: 1) motivation, 2) attitudes, 3-4, tied) experience/retention, 5) perception, 6-8, tied) interest/identity/emotion, 9) values, 10) understanding
#post values ranked: 1) emotion, 2-4, tied) values/interests/attitudes, 5) motivation, 6) perception, 7-9, tied) identity/interest/retention, 10) understanding

#First analysis - whether or not treatment was effective and for what type of teaching outcome:
#The following 6 teaching outcomes were significantly different when comparing pre-post scores; i.e., a positive effect due to treatment (produced a figure that shows this): 
#[ranked from greater-lower mean difference] 1) values, 2) emotions, 3) perceptions, 4) identity, 5) attitude, and 6) experience.
#The following 4 outcomes were not met nor significant; i.e. a neutral effect due to treatment: 
#[ranked similarly as before] 1) interest, 2) understanding, 3) retention, and 4) motivation.
#These were validated by executing [parametric] paired t-tests and [non-parametric] wilcoxon rank-sum tests. Pre-post data difference was tested for normality through Shapiro-Wilk analyses. 

#Given that the data was generally uniform among responses/effects (shown my positive mean differences, similar data spreads), there was not significant differences between any of the teaching outcomes.
#Thus, there is no highest*lowest/pre-post interactions able to be explained. The design proposed likely limited the potential to pose this hypothesis.
#But, this can be validated by executing an ANCOVA which accounts for predicting post-test response while considering the learning type and [underlying effect] of post-scores.
